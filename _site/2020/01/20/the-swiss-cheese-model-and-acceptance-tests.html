<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>The swiss cheese model and acceptance tests - Korny’s Blog</title>
<meta name="description" content="Testing and Agile I’ve seen testing, especially acceptance testing, done horribly wrongly over the years, and sadly I often see the same anti-patterns repeated over and over; so I thought it’d be worth talking about my perspectives on this thorny subject.  First off, I think it’s worth emphasising that automated testing is at the heart of agile software development.  Agile development assumes “building quality in”, and that means tests. I’ve seen places with no tests, or nothing but a suite of slow fragile end-to-end tests run on a snowflake environment, who want to “go agile”.  You can’t just “go agile” without a solid underpinning of low-level tests that enable you to move fast without breaking things.  Going agile without good tests is like embracing modern surgery but skipping all that pointless handwashing stuff.  All of this is doubly true when moving towards continuous delivery - I strongly recommend people read the “Technical Practices” chapter in “Accelerate” 1 (really, just go read the whole book - it’s awesome, providing the data behind the benefits of agile and CD).  The test pyramid  A classic metaphor in automated testing is the test pyramid:    it’s been a useful model for ages, and still a good conversation starter, but it lacks a few things:     It isn’t clear really what the horizontal dimension is - does wider mean more tests? More test scenarios? More features tested?   It isn’t clear what the vertical dimension is - where do contract tests fit? What about chaos engineering / resiliency tests?  What if you test at an API not a UI?  Should you test in a particular order?   In many cases the best shape is nothing like a pyramid - some systems are well suited to integration tests, and tend to a pear-shape.  Some systems are more of an hourglass, with lots of UI/API tests and lots of unit tests and not much between.   I’ve seen people tweak the pyramid - adding layers, adding axes, adding explanations - but fundamentally I think it’s a bit flawed.  A different perspective - Swiss Cheese  I recently came across a new-ish metaphor: the Swiss Cheese model of testing - which I quite like. It’s a lot more nuanced than the old Test Pyramid, and helpful when it comes to talking about both why we test, and how we should test.  The basic idea is: consider your tests like a big stack of swiss cheese slices - you know, the kind with holes in them:    Now layer those cheese slices vertically - each layer represents a different kind of tests.  Order them in the order you run them - usually simplest, fastest feedback first, then slower layers below them:    You can imagine defects as physical bugs which fall down the diagram, and are caught at different levels - different slices of cheese.  Some bugs might fall all the way through a series of holes and not get caught.  This is bad.    Each layer, and each kind of test, has different tradeoffs such as:     How fast is the feedback cycle? Can developers know they have a problem as they type (e.g. with a linter), or as soon as they save a file, or when they run a 10 second test suite? Or do they have to wait until the evening after they pushed their code changes?  Continuous Integration is what we’d like - every commit runs every test - but some layers are going to be slow or expensive.   How fragile are the tests? - do they fail in confusing and obscure ways? Are they reliable, failing the same way from the same problem? Or failing the same way from different problems?   How expensive are the tests? - do they take a lot of effort to write? Do they need re-writing whenever you change code?   And each layer has holes - things that are not sensible to test at that level.  You don’t test your exception handling in a browser test.  You don’t test your microservice interactions in a unit test.  Note I haven’t prescribed what the precise test phases are - there’s a lot of “it depends” on choosing your tests.  There’s a whole other blog post to be written about my preferred test layers!  A basic principle here though is - don’t repeat yourself. There’s not a lot of value testing the same thing multiple times.  The cheese layers will always overlap a bit - but if you have a fast simple unit test to verify the text of a validation message, don’t also have a slow fragile browser-based end-to-end test that verifies the same thing.  This is especially true of manual tests - the typical top of the classical test pyramid.  You shouldn’t manually re-test what you have automatically tested.  You might “kick the tyres” to make sure that everything works OK.  You might test in production, relying on monitoring and A/B testing to identify problems early.  But you really want to catch everything you can before that point.  Also it’s somewhat up to you to decide what is most appropriate for which layer.  Fast feedback is good, but so are clear expressive tests that are easy to change.  A classic example is database interactions - it’s almost always better to test against a real database, possibly an in-memory one, than to try to mock it out.  Acceptance tests  One “cheese slice” I definitely don’t want to see is the “acceptance tests” slice.  Don’t get me wrong, I love acceptance tests, I love the idea of defining “done” for a user story by clear unambiguous tests.  But too many people assume there must be an accepance test stage - a single “slice” in this model - which contains all the acceptance tests.  This means those tests usually end up near the bottom of the pile, in a suite of large slow expensive browser-driven tests, that have to run against a production-like environment.  Sure, that’s appealing - lots of us started like that, learning the magic of BDD and Cucumber and Selenium, building amazing suites of exhaustive browser-based tests.  But that magic was slow, and fragile - and no amount of tinkering with clever setups and special approaches got past the fact that the tests took far far too long to run, and were fragile, and hampered rapid development.  Besides, they encouraged people to only think of “acceptance” in terms of “clicks and buttons” - where were the tests for “it should handle network failures gracefully” or “it should send exception logs to the auditing system”?  (I do think there’s value in some browser-driven end-to-end tests - there are bugs you can only catch that way.  But call them “smoke tests” or “end-to-end tests” or something, not “acceptance tests”.  If you want lots of tests of UI features, consider tools that test within the framework you use, like enzyme for react - you might want several “cheese slices” of UI tests - see how the material UI tests are layered for an example.)  In my opinion, acceptance criteria should be tested at whatever level is most effective for testing that requirement:    If the acceptance criterion is “it should have the title Fnord Motor Company” then that can be a simple browser-based test against a stubbed back end.  If the acceptance criterion is “it should not accept a password shorter than 10 characters” then that might be a pair of unit tests, one to check that the UI validation is good, and one to check that the server-side validation is good. If the acceptance criterion is  “it should respond within 30ms under peak load” then that might be part of a performance test suite.  If you really feel the need to trace acceptance tests back to stories, you can probably work out a way to tag the tests and report on them somehow - but I’d ask, why bother?  Are you ever going to use that information?  Maybe it’s sufficient to just list the tests in the stories, and check they are there as part of signing the story as done, and not try to track the relationship beyond that.  User stories are a point in time, tests are forever  Another problem with an Acceptance Test layer, is that user stories express the requirements at the time the story was written - usually with changes right up to the point of delivery.  At the time of finishing a story, acceptance tests will demonstrate it works - but the next story, or the one after that, will change the behaviour.  If your tests are too strongly structured around the stories, they can hamper change - you need to think “OK, it’s now the Flawed Motor Company, I need to find all the old acceptance criteria that might have tested for this”.  And again, this is especially time consuming if that test is in a UI-based test suite which takes a long time to run and is hard to debug.  If your acceptance test was just a note in the original story “this is demonstrated by the view-layer test XYZ” then you wouldn’t need to change anything except the single failing test - if there was ever a pressing need to prove the original acceptance criteria were checked,  a code archeologist could dig through version control history to find the test XYZ at the time the story was completed.  The QA role and test slices  This also highlights a common organisational problem - when there is an individual or team whose role is entirely Quality Assurance, and who tests independently to the rest of the team.  It is only natural that this kind of QA will tend to work at a single layer - without close collaboration with developers, it is next to impossible to work with unit test or other fast feedback layers.  And this is a reason why you shouldn’t test this way!  Don’t get me wrong, I’m not advocating getting rid of QA folks - a good QA specialist can be an invaluable team member.  But they should be working with the developers, to encourage them to write good tests, to spot the gaps in their test automation, and to provide a vital whole-project perspective on quality.  The “Accelerate” 1 folks put it better than I can:     [context: talking about qualities of high performing teams]    Developers primarily create and maintain acceptance tests, and they can easily reproduce and fix them on their development workstations.    It is interesting to note that having automated tests primarily created and maintained either by QA or an outsourced party is not correlated with IT performance.    The theory behind this is that when developers are involved in creating and maintaining acceptance tests, there are two important effects. First, the code becomes more testable when developers write tests. This is one of the main reasons why test-driven development (TDD) is an important practice‚ as it forces developers to create more testable designs. Second, when developers are responsible for the automated tests, they care more about them and will invest more effort into maintaining and fixing them.       None of this means that we should be getting rid of testers.  Testers serve an essential role in the software delivery lifecycle, performing manual testing such as exploratory, usability, and acceptance testing, and helping to create and evolve suites of automated tests by working alongside developers.   The best QAs I’ve worked with were awesome assets to the team.  They didn’t write that many tests on their own though - instead they worked closely with developers to ensure that they understood their tests, that the tests covered as many cheese holes as possible, and that the test fitted in with their big-picture vision of the overall testing suite - Were tests at the right level? Was enough being tested?  Were acceptance criteria being met? And what areas would still need exploratory testing?  In summary     Write tests at as low a level as is sensible.   Write tests that cover all the things that could go wrong.  Where you can.   Don’t repeat yourself!   Integrate continuously - and run all the tests on each commit.  Or as many as you can.   Define acceptance criteria, and write acceptance tests, at the lowest level that makes sense.   Let the QAs overview the tests, let the developers write them, work as a team.   Comments? Questions? See the comments section at the bottom of this page.    Various references:  Wikipedia has an article about this metaphor in accident causation - this seems to date the idea back to 1990, when looking at layered threat mitigations  Glennan Carnie - the Swiss Cheese Model is a good introduction to the metaphor, but seems to talk more about static analysis than kinds of tests  My colleague Sarah Hutchins wrote this article recently about the Swiss Cheese Model - with a bit of a different focus, looking at strategic decisions about what to test where. Vinod Kumaar R’s article has more useful perspectives.  Martin Fowler has a good basic article about the Test Pyramid in his bliki, and the same site also has Ham Vocke’s article with more practical examples  I also quite like Gregory Paciga’s “testing is like a box of rocks” metaphor  Image sources:  Swiss cheese image by Ekg917, CC BY-SA 4.0  Test Pyramid image by Abbe98 [CC BY-SA 4.0], via Wikimedia Commons  Thanks:     to my colleagues Nishi Ningegowda, Jim Barritt, Steve Aylward, and of course Becky Thorn Sietsma for their useful feedback!   Footnotes:                 Nicole Forsgren, Jez Humble &amp; Gene Kim - Accelerate https://itrevolution.com/book/accelerate/ &#8617; &#8617;2">


  <meta name="author" content="Korny Sietsma">
  
  <meta property="article:author" content="Korny Sietsma">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Korny's Blog">
<meta property="og:title" content="The swiss cheese model and acceptance tests">
<meta property="og:url" content="http://localhost:4000/2020/01/20/the-swiss-cheese-model-and-acceptance-tests.html">


  <meta property="og:description" content="Testing and Agile I’ve seen testing, especially acceptance testing, done horribly wrongly over the years, and sadly I often see the same anti-patterns repeated over and over; so I thought it’d be worth talking about my perspectives on this thorny subject.  First off, I think it’s worth emphasising that automated testing is at the heart of agile software development.  Agile development assumes “building quality in”, and that means tests. I’ve seen places with no tests, or nothing but a suite of slow fragile end-to-end tests run on a snowflake environment, who want to “go agile”.  You can’t just “go agile” without a solid underpinning of low-level tests that enable you to move fast without breaking things.  Going agile without good tests is like embracing modern surgery but skipping all that pointless handwashing stuff.  All of this is doubly true when moving towards continuous delivery - I strongly recommend people read the “Technical Practices” chapter in “Accelerate” 1 (really, just go read the whole book - it’s awesome, providing the data behind the benefits of agile and CD).  The test pyramid  A classic metaphor in automated testing is the test pyramid:    it’s been a useful model for ages, and still a good conversation starter, but it lacks a few things:     It isn’t clear really what the horizontal dimension is - does wider mean more tests? More test scenarios? More features tested?   It isn’t clear what the vertical dimension is - where do contract tests fit? What about chaos engineering / resiliency tests?  What if you test at an API not a UI?  Should you test in a particular order?   In many cases the best shape is nothing like a pyramid - some systems are well suited to integration tests, and tend to a pear-shape.  Some systems are more of an hourglass, with lots of UI/API tests and lots of unit tests and not much between.   I’ve seen people tweak the pyramid - adding layers, adding axes, adding explanations - but fundamentally I think it’s a bit flawed.  A different perspective - Swiss Cheese  I recently came across a new-ish metaphor: the Swiss Cheese model of testing - which I quite like. It’s a lot more nuanced than the old Test Pyramid, and helpful when it comes to talking about both why we test, and how we should test.  The basic idea is: consider your tests like a big stack of swiss cheese slices - you know, the kind with holes in them:    Now layer those cheese slices vertically - each layer represents a different kind of tests.  Order them in the order you run them - usually simplest, fastest feedback first, then slower layers below them:    You can imagine defects as physical bugs which fall down the diagram, and are caught at different levels - different slices of cheese.  Some bugs might fall all the way through a series of holes and not get caught.  This is bad.    Each layer, and each kind of test, has different tradeoffs such as:     How fast is the feedback cycle? Can developers know they have a problem as they type (e.g. with a linter), or as soon as they save a file, or when they run a 10 second test suite? Or do they have to wait until the evening after they pushed their code changes?  Continuous Integration is what we’d like - every commit runs every test - but some layers are going to be slow or expensive.   How fragile are the tests? - do they fail in confusing and obscure ways? Are they reliable, failing the same way from the same problem? Or failing the same way from different problems?   How expensive are the tests? - do they take a lot of effort to write? Do they need re-writing whenever you change code?   And each layer has holes - things that are not sensible to test at that level.  You don’t test your exception handling in a browser test.  You don’t test your microservice interactions in a unit test.  Note I haven’t prescribed what the precise test phases are - there’s a lot of “it depends” on choosing your tests.  There’s a whole other blog post to be written about my preferred test layers!  A basic principle here though is - don’t repeat yourself. There’s not a lot of value testing the same thing multiple times.  The cheese layers will always overlap a bit - but if you have a fast simple unit test to verify the text of a validation message, don’t also have a slow fragile browser-based end-to-end test that verifies the same thing.  This is especially true of manual tests - the typical top of the classical test pyramid.  You shouldn’t manually re-test what you have automatically tested.  You might “kick the tyres” to make sure that everything works OK.  You might test in production, relying on monitoring and A/B testing to identify problems early.  But you really want to catch everything you can before that point.  Also it’s somewhat up to you to decide what is most appropriate for which layer.  Fast feedback is good, but so are clear expressive tests that are easy to change.  A classic example is database interactions - it’s almost always better to test against a real database, possibly an in-memory one, than to try to mock it out.  Acceptance tests  One “cheese slice” I definitely don’t want to see is the “acceptance tests” slice.  Don’t get me wrong, I love acceptance tests, I love the idea of defining “done” for a user story by clear unambiguous tests.  But too many people assume there must be an accepance test stage - a single “slice” in this model - which contains all the acceptance tests.  This means those tests usually end up near the bottom of the pile, in a suite of large slow expensive browser-driven tests, that have to run against a production-like environment.  Sure, that’s appealing - lots of us started like that, learning the magic of BDD and Cucumber and Selenium, building amazing suites of exhaustive browser-based tests.  But that magic was slow, and fragile - and no amount of tinkering with clever setups and special approaches got past the fact that the tests took far far too long to run, and were fragile, and hampered rapid development.  Besides, they encouraged people to only think of “acceptance” in terms of “clicks and buttons” - where were the tests for “it should handle network failures gracefully” or “it should send exception logs to the auditing system”?  (I do think there’s value in some browser-driven end-to-end tests - there are bugs you can only catch that way.  But call them “smoke tests” or “end-to-end tests” or something, not “acceptance tests”.  If you want lots of tests of UI features, consider tools that test within the framework you use, like enzyme for react - you might want several “cheese slices” of UI tests - see how the material UI tests are layered for an example.)  In my opinion, acceptance criteria should be tested at whatever level is most effective for testing that requirement:    If the acceptance criterion is “it should have the title Fnord Motor Company” then that can be a simple browser-based test against a stubbed back end.  If the acceptance criterion is “it should not accept a password shorter than 10 characters” then that might be a pair of unit tests, one to check that the UI validation is good, and one to check that the server-side validation is good. If the acceptance criterion is  “it should respond within 30ms under peak load” then that might be part of a performance test suite.  If you really feel the need to trace acceptance tests back to stories, you can probably work out a way to tag the tests and report on them somehow - but I’d ask, why bother?  Are you ever going to use that information?  Maybe it’s sufficient to just list the tests in the stories, and check they are there as part of signing the story as done, and not try to track the relationship beyond that.  User stories are a point in time, tests are forever  Another problem with an Acceptance Test layer, is that user stories express the requirements at the time the story was written - usually with changes right up to the point of delivery.  At the time of finishing a story, acceptance tests will demonstrate it works - but the next story, or the one after that, will change the behaviour.  If your tests are too strongly structured around the stories, they can hamper change - you need to think “OK, it’s now the Flawed Motor Company, I need to find all the old acceptance criteria that might have tested for this”.  And again, this is especially time consuming if that test is in a UI-based test suite which takes a long time to run and is hard to debug.  If your acceptance test was just a note in the original story “this is demonstrated by the view-layer test XYZ” then you wouldn’t need to change anything except the single failing test - if there was ever a pressing need to prove the original acceptance criteria were checked,  a code archeologist could dig through version control history to find the test XYZ at the time the story was completed.  The QA role and test slices  This also highlights a common organisational problem - when there is an individual or team whose role is entirely Quality Assurance, and who tests independently to the rest of the team.  It is only natural that this kind of QA will tend to work at a single layer - without close collaboration with developers, it is next to impossible to work with unit test or other fast feedback layers.  And this is a reason why you shouldn’t test this way!  Don’t get me wrong, I’m not advocating getting rid of QA folks - a good QA specialist can be an invaluable team member.  But they should be working with the developers, to encourage them to write good tests, to spot the gaps in their test automation, and to provide a vital whole-project perspective on quality.  The “Accelerate” 1 folks put it better than I can:     [context: talking about qualities of high performing teams]    Developers primarily create and maintain acceptance tests, and they can easily reproduce and fix them on their development workstations.    It is interesting to note that having automated tests primarily created and maintained either by QA or an outsourced party is not correlated with IT performance.    The theory behind this is that when developers are involved in creating and maintaining acceptance tests, there are two important effects. First, the code becomes more testable when developers write tests. This is one of the main reasons why test-driven development (TDD) is an important practice‚ as it forces developers to create more testable designs. Second, when developers are responsible for the automated tests, they care more about them and will invest more effort into maintaining and fixing them.       None of this means that we should be getting rid of testers.  Testers serve an essential role in the software delivery lifecycle, performing manual testing such as exploratory, usability, and acceptance testing, and helping to create and evolve suites of automated tests by working alongside developers.   The best QAs I’ve worked with were awesome assets to the team.  They didn’t write that many tests on their own though - instead they worked closely with developers to ensure that they understood their tests, that the tests covered as many cheese holes as possible, and that the test fitted in with their big-picture vision of the overall testing suite - Were tests at the right level? Was enough being tested?  Were acceptance criteria being met? And what areas would still need exploratory testing?  In summary     Write tests at as low a level as is sensible.   Write tests that cover all the things that could go wrong.  Where you can.   Don’t repeat yourself!   Integrate continuously - and run all the tests on each commit.  Or as many as you can.   Define acceptance criteria, and write acceptance tests, at the lowest level that makes sense.   Let the QAs overview the tests, let the developers write them, work as a team.   Comments? Questions? See the comments section at the bottom of this page.    Various references:  Wikipedia has an article about this metaphor in accident causation - this seems to date the idea back to 1990, when looking at layered threat mitigations  Glennan Carnie - the Swiss Cheese Model is a good introduction to the metaphor, but seems to talk more about static analysis than kinds of tests  My colleague Sarah Hutchins wrote this article recently about the Swiss Cheese Model - with a bit of a different focus, looking at strategic decisions about what to test where. Vinod Kumaar R’s article has more useful perspectives.  Martin Fowler has a good basic article about the Test Pyramid in his bliki, and the same site also has Ham Vocke’s article with more practical examples  I also quite like Gregory Paciga’s “testing is like a box of rocks” metaphor  Image sources:  Swiss cheese image by Ekg917, CC BY-SA 4.0  Test Pyramid image by Abbe98 [CC BY-SA 4.0], via Wikimedia Commons  Thanks:     to my colleagues Nishi Ningegowda, Jim Barritt, Steve Aylward, and of course Becky Thorn Sietsma for their useful feedback!   Footnotes:                 Nicole Forsgren, Jez Humble &amp; Gene Kim - Accelerate https://itrevolution.com/book/accelerate/ &#8617; &#8617;2">







  <meta property="article:published_time" content="2020-01-20T08:13:00+00:00">






<link rel="canonical" href="http://localhost:4000/2020/01/20/the-swiss-cheese-model-and-acceptance-tests.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Korny's Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Korny's Blog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/korny-face.png" alt="Korny Sietsma" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Korny Sietsma</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Geek, Parent, Coder, Aussie living in the UK</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://korny.info" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Korny.info</span></a></li>
          
        
          
            <li><a href="https://hachyderm.io/@korny" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-mastodon" aria-hidden="true"></i><span class="label">Mastodon</span></a></li>
          
        
          
            <li><a href="https://github.com/kornysietsma" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="The swiss cheese model and acceptance tests">
    <meta itemprop="description" content="Testing and AgileI’ve seen testing, especially acceptance testing, done horribly wrongly over the years, and sadly I often see the same anti-patterns repeated over and over; so I thought it’d be worth talking about my perspectives on this thorny subject.First off, I think it’s worth emphasising that automated testing is at the heart of agile software development.  Agile development assumes “building quality in”, and that means tests. I’ve seen places with no tests, or nothing but a suite of slow fragile end-to-end tests run on a snowflake environment, who want to “go agile”.  You can’t just “go agile” without a solid underpinning of low-level tests that enable you to move fast without breaking things.  Going agile without good tests is like embracing modern surgery but skipping all that pointless handwashing stuff.All of this is doubly true when moving towards continuous delivery - I strongly recommend people read the “Technical Practices” chapter in “Accelerate” 1 (really, just go read the whole book - it’s awesome, providing the data behind the benefits of agile and CD).The test pyramidA classic metaphor in automated testing is the test pyramid:it’s been a useful model for ages, and still a good conversation starter, but it lacks a few things:  It isn’t clear really what the horizontal dimension is - does wider mean more tests? More test scenarios? More features tested?  It isn’t clear what the vertical dimension is - where do contract tests fit? What about chaos engineering / resiliency tests?  What if you test at an API not a UI?  Should you test in a particular order?  In many cases the best shape is nothing like a pyramid - some systems are well suited to integration tests, and tend to a pear-shape.  Some systems are more of an hourglass, with lots of UI/API tests and lots of unit tests and not much between.I’ve seen people tweak the pyramid - adding layers, adding axes, adding explanations - but fundamentally I think it’s a bit flawed.A different perspective - Swiss CheeseI recently came across a new-ish metaphor: the Swiss Cheese model of testing - which I quite like. It’s a lot more nuanced than the old Test Pyramid, and helpful when it comes to talking about both why we test, and how we should test.The basic idea is: consider your tests like a big stack of swiss cheese slices - you know, the kind with holes in them:Now layer those cheese slices vertically - each layer represents a different kind of tests.  Order them in the order you run them - usually simplest, fastest feedback first, then slower layers below them:You can imagine defects as physical bugs which fall down the diagram, and are caught at different levels - different slices of cheese.Some bugs might fall all the way through a series of holes and not get caught.  This is bad.Each layer, and each kind of test, has different tradeoffs such as:  How fast is the feedback cycle? Can developers know they have a problem as they type (e.g. with a linter), or as soon as they save a file, or when they run a 10 second test suite? Or do they have to wait until the evening after they pushed their code changes?  Continuous Integration is what we’d like - every commit runs every test - but some layers are going to be slow or expensive.  How fragile are the tests? - do they fail in confusing and obscure ways? Are they reliable, failing the same way from the same problem? Or failing the same way from different problems?  How expensive are the tests? - do they take a lot of effort to write? Do they need re-writing whenever you change code?And each layer has holes - things that are not sensible to test at that level.  You don’t test your exception handling in a browser test.  You don’t test your microservice interactions in a unit test.Note I haven’t prescribed what the precise test phases are - there’s a lot of “it depends” on choosing your tests.  There’s a whole other blog post to be written about my preferred test layers!A basic principle here though is - don’t repeat yourself. There’s not a lot of value testing the same thing multiple times.  The cheese layers will always overlap a bit - but if you have a fast simple unit test to verify the text of a validation message, don’t also have a slow fragile browser-based end-to-end test that verifies the same thing.This is especially true of manual tests - the typical top of the classical test pyramid.  You shouldn’t manually re-test what you have automatically tested.  You might “kick the tyres” to make sure that everything works OK.  You might test in production, relying on monitoring and A/B testing to identify problems early.  But you really want to catch everything you can before that point.Also it’s somewhat up to you to decide what is most appropriate for which layer.  Fast feedback is good, but so are clear expressive tests that are easy to change.  A classic example is database interactions - it’s almost always better to test against a real database, possibly an in-memory one, than to try to mock it out.Acceptance testsOne “cheese slice” I definitely don’t want to see is the “acceptance tests” slice.  Don’t get me wrong, I love acceptance tests, I love the idea of defining “done” for a user story by clear unambiguous tests.But too many people assume there must be an accepance test stage - a single “slice” in this model - which contains all the acceptance tests.  This means those tests usually end up near the bottom of the pile, in a suite of large slow expensive browser-driven tests, that have to run against a production-like environment.Sure, that’s appealing - lots of us started like that, learning the magic of BDD and Cucumber and Selenium, building amazing suites of exhaustive browser-based tests.  But that magic was slow, and fragile - and no amount of tinkering with clever setups and special approaches got past the fact that the tests took far far too long to run, and were fragile, and hampered rapid development.Besides, they encouraged people to only think of “acceptance” in terms of “clicks and buttons” - where were the tests for “it should handle network failures gracefully” or “it should send exception logs to the auditing system”?(I do think there’s value in some browser-driven end-to-end tests - there are bugs you can only catch that way.  But call them “smoke tests” or “end-to-end tests” or something, not “acceptance tests”.  If you want lots of tests of UI features, consider tools that test within the framework you use, like enzyme for react - you might want several “cheese slices” of UI tests - see how the material UI tests are layered for an example.)In my opinion, acceptance criteria should be tested at whatever level is most effective for testing that requirement:If the acceptance criterion is “it should have the title Fnord Motor Company” then that can be a simple browser-based test against a stubbed back end.  If the acceptance criterion is “it should not accept a password shorter than 10 characters” then that might be a pair of unit tests, one to check that the UI validation is good, and one to check that the server-side validation is good. If the acceptance criterion is  “it should respond within 30ms under peak load” then that might be part of a performance test suite.If you really feel the need to trace acceptance tests back to stories, you can probably work out a way to tag the tests and report on them somehow - but I’d ask, why bother?  Are you ever going to use that information?  Maybe it’s sufficient to just list the tests in the stories, and check they are there as part of signing the story as done, and not try to track the relationship beyond that.User stories are a point in time, tests are foreverAnother problem with an Acceptance Test layer, is that user stories express the requirements at the time the story was written - usually with changes right up to the point of delivery.  At the time of finishing a story, acceptance tests will demonstrate it works - but the next story, or the one after that, will change the behaviour.  If your tests are too strongly structured around the stories, they can hamper change - you need to think “OK, it’s now the Flawed Motor Company, I need to find all the old acceptance criteria that might have tested for this”.And again, this is especially time consuming if that test is in a UI-based test suite which takes a long time to run and is hard to debug.If your acceptance test was just a note in the original story “this is demonstrated by the view-layer test XYZ” then you wouldn’t need to change anything except the single failing test - if there was ever a pressing need to prove the original acceptance criteria were checked,  a code archeologist could dig through version control history to find the test XYZ at the time the story was completed.The QA role and test slicesThis also highlights a common organisational problem - when there is an individual or team whose role is entirely Quality Assurance, and who tests independently to the rest of the team.It is only natural that this kind of QA will tend to work at a single layer - without close collaboration with developers, it is next to impossible to work with unit test or other fast feedback layers.And this is a reason why you shouldn’t test this way!  Don’t get me wrong, I’m not advocating getting rid of QA folks - a good QA specialist can be an invaluable team member.  But they should be working with the developers, to encourage them to write good tests, to spot the gaps in their test automation, and to provide a vital whole-project perspective on quality.The “Accelerate” 1 folks put it better than I can:  [context: talking about qualities of high performing teams]  Developers primarily create and maintain acceptance tests, and they can easilyreproduce and fix them on their development workstations.  It is interesting to notethat having automated tests primarily created and maintained either by QA or anoutsourced party is not correlated with IT performance.  The theory behind this isthat when developers are involved in creating and maintaining acceptance tests,there are two important effects. First, the code becomes more testable whendevelopers write tests. This is one of the main reasons why test-driven development(TDD) is an important practice‚ as it forces developers to create more testable designs.Second, when developers are responsible for the automated tests, they care moreabout them and will invest more effort into maintaining and fixing them.  None of this means that we should be getting rid of testers.  Testers serve an essential role in the software delivery lifecycle, performing manual testing such as exploratory, usability, and acceptance testing, and helping to create and evolve suites of automated tests by working alongside developers.The best QAs I’ve worked with were awesome assets to the team.  They didn’t write that many tests on their own though - instead they worked closely with developers to ensure that they understood their tests, that the tests covered as many cheese holesas possible, and that the test fitted in with their big-picture vision of the overall testing suite - Were tests at the right level? Was enough being tested?  Were acceptance criteria being met? And what areas would still need exploratory testing?In summary  Write tests at as low a level as is sensible.  Write tests that cover all the things that could go wrong.  Where you can.  Don’t repeat yourself!  Integrate continuously - and run all the tests on each commit.  Or as many as you can.  Define acceptance criteria, and write acceptance tests, at the lowest level that makes sense.  Let the QAs overview the tests, let the developers write them, work as a team.Comments? Questions? See the comments section at the bottom of this page.Various references:Wikipedia has an article about this metaphor in accident causation - this seems to date the idea back to 1990, when looking at layered threat mitigationsGlennan Carnie - the Swiss Cheese Model is a good introduction to the metaphor, but seems to talk more about static analysis than kinds of testsMy colleague Sarah Hutchins wrote this article recently about the Swiss Cheese Model - with a bit of a different focus, looking at strategic decisions about what to test where. Vinod Kumaar R’s article has more useful perspectives.Martin Fowler has a good basic article about the Test Pyramid in his bliki, and the same site also has Ham Vocke’s article with more practical examplesI also quite like Gregory Paciga’s “testing is like a box of rocks” metaphorImage sources:Swiss cheese image by Ekg917, CC BY-SA 4.0Test Pyramid image by Abbe98 [CC BY-SA 4.0], via Wikimedia CommonsThanks:  to my colleagues Nishi Ningegowda, Jim Barritt, Steve Aylward, and of course Becky Thorn Sietsma for their useful feedback!Footnotes:            Nicole Forsgren, Jez Humble &amp; Gene Kim - Accelerate https://itrevolution.com/book/accelerate/ &#8617; &#8617;2      ">
    <meta itemprop="datePublished" content="2020-01-20T08:13:00+00:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/2020/01/20/the-swiss-cheese-model-and-acceptance-tests.html" class="u-url" itemprop="url">The swiss cheese model and acceptance tests
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2020-01-20T08:13:00+00:00">January 20, 2020</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#testing-and-agile">Testing and Agile</a></li><li><a href="#the-test-pyramid">The test pyramid</a></li><li><a href="#a-different-perspective---swiss-cheese">A different perspective - Swiss Cheese</a></li><li><a href="#user-stories-are-a-point-in-time-tests-are-forever">User stories are a point in time, tests are forever</a></li><li><a href="#the-qa-role-and-test-slices">The QA role and test slices</a></li><li><a href="#various-references">Various references:</a></li><li><a href="#image-sources">Image sources:</a></li><li><a href="#thanks">Thanks:</a></li><li><a href="#footnotes">Footnotes:</a></li></ul>

            </nav>
          </aside>
        
        <h2 id="testing-and-agile">Testing and Agile</h2>
<p>I’ve seen testing, especially acceptance testing, done horribly wrongly over the years, and sadly I often see the same anti-patterns repeated over and over; so I thought it’d be worth talking about my perspectives on this thorny subject.</p>

<p>First off, I think it’s worth emphasising that <em>automated testing is at the heart of agile software development</em>.  Agile development assumes “building quality in”, and that means tests. I’ve seen places with <em>no</em> tests, or nothing but a suite of slow fragile end-to-end tests run on a snowflake environment, who want to “go agile”.  You can’t just “go agile” without a solid underpinning of low-level tests that <em>enable</em> you to move fast without breaking things.  Going agile without good tests is like embracing modern surgery but skipping all that pointless handwashing stuff.</p>

<p>All of this is doubly true when moving towards continuous delivery - I strongly recommend people read the “Technical Practices” chapter in “<em>Accelerate</em>” <sup id="fnref:accelerate" role="doc-noteref"><a href="#fn:accelerate" class="footnote" rel="footnote">1</a></sup> (really, just go read the whole book - it’s awesome, providing the <em>data</em> behind the benefits of agile and CD).</p>

<h2 id="the-test-pyramid">The test pyramid</h2>

<p>A classic metaphor in automated testing is <a href="https://martinfowler.com/bliki/TestPyramid.html">the test pyramid:</a></p>

<p><img src="/assets/images/2019-07-22-the-swiss-cheese-model-and-acceptance-tests/Testing_Pyramid.svg" alt="test pyramid" /></p>

<p>it’s been a useful model for ages, and still a good conversation starter, but it lacks a few things:</p>

<ul>
  <li>It isn’t clear really what the horizontal dimension is - does wider mean more tests? More test scenarios? More features tested?</li>
  <li>It isn’t clear what the vertical dimension is - where do contract tests fit? What about chaos engineering / resiliency tests?  What if you test at an API not a UI?  Should you test in a particular order?</li>
  <li>In many cases the best shape is nothing like a pyramid - some systems are well suited to integration tests, and tend to a pear-shape.  Some systems are more of an hourglass, with lots of UI/API tests and lots of unit tests and not much between.</li>
</ul>

<p>I’ve seen people tweak the pyramid - adding layers, adding axes, adding explanations - but fundamentally I think it’s a bit flawed.</p>

<h2 id="a-different-perspective---swiss-cheese">A different perspective - Swiss Cheese</h2>

<p>I recently came across a new-ish metaphor: the Swiss Cheese model of testing - which I quite like. It’s a lot more nuanced than the old Test Pyramid, and helpful when it comes to talking about both <em>why</em> we test, and <em>how</em> we should test.</p>

<p>The basic idea is: consider your tests like a big stack of swiss cheese slices - you know, the kind with holes in them:</p>

<p><img src="/assets/images/2019-07-22-the-swiss-cheese-model-and-acceptance-tests/Swiss_cheese.jpg" alt="swiss cheese" /></p>

<p>Now layer those cheese slices vertically - each layer represents a different kind of tests.  Order them in the order you run them - usually simplest, fastest feedback first, then slower layers below them:</p>

<p><img src="/assets/images/2019-07-22-the-swiss-cheese-model-and-acceptance-tests/test_swiss_cheese.svg" alt="swiss cheese slices" /></p>

<p>You can imagine defects as physical bugs which fall down the diagram, and are caught at different levels - different slices of cheese.</p>

<p>Some bugs might fall all the way through a series of holes and not get caught.  This is bad.</p>

<p><img src="/assets/images/2019-07-22-the-swiss-cheese-model-and-acceptance-tests/bug_catching.svg" alt="swiss cheese slices" /></p>

<p>Each layer, and each kind of test, has different tradeoffs such as:</p>

<ul>
  <li><em>How fast is the feedback cycle?</em> Can developers know they have a problem <em>as they type</em> (e.g. with a linter), or as soon as they save a file, or <a href="https://blog.ploeh.dk/2012/05/24/TDDtestsuitesshouldrunin10secondsorless/">when they run a 10 second test suite</a>? Or do they have to wait until the evening after they pushed their code changes?  Continuous Integration is what we’d like - every commit runs every test - but some layers are going to be slow or expensive.</li>
  <li><em>How fragile are the tests?</em> - do they fail in confusing and obscure ways? Are they reliable, failing the same way from the same problem? Or failing the same way from different problems?</li>
  <li><em>How expensive are the tests?</em> - do they take a lot of effort to write? Do they need re-writing whenever you change code?</li>
</ul>

<p>And each layer has holes - things that are not sensible to test at that level.  You don’t test your exception handling in a browser test.  You don’t test your microservice interactions in a unit test.</p>

<p><em>Note</em> I haven’t prescribed what the precise test phases are - there’s a lot of “it depends” on choosing your tests.  There’s a whole other blog post to be written about my preferred test layers!</p>

<p>A basic principle here though is - <em>don’t repeat yourself</em>. There’s not a lot of value testing the same thing multiple times.  The cheese layers will always overlap a bit - but if you have a fast simple unit test to verify the text of a validation message, don’t also have a slow fragile browser-based end-to-end test that verifies the same thing.</p>

<p>This is especially true of manual tests - the typical top of the classical test pyramid.  You shouldn’t manually re-test what you have automatically tested.  You might “kick the tyres” to make sure that everything works OK.  You might test in production, relying on monitoring and A/B testing to identify problems early.  But you really want to catch everything you can before that point.</p>

<p>Also it’s somewhat up to you to decide what is most appropriate for which layer.  Fast feedback is good, but so are clear expressive tests that are easy to change.  A classic example is database interactions - it’s almost always better to test against a real database, possibly an in-memory one, than to try to mock it out.</p>

<h1 id="acceptance-tests">Acceptance tests</h1>

<p>One “cheese slice” I definitely <em>don’t</em> want to see is the “acceptance tests” slice.  Don’t get me wrong, I love acceptance tests, I love the idea of defining “done” for a user story by clear unambiguous tests.</p>

<p>But too many people assume there must be an accepance test stage - a single “slice” in this model - which contains all the acceptance tests.  This means those tests usually end up near the bottom of the pile, in a suite of large slow expensive browser-driven tests, that have to run against a production-like environment.</p>

<p>Sure, that’s appealing - lots of us started like that, learning the magic of BDD and Cucumber and Selenium, building amazing suites of exhaustive browser-based tests.  But that magic was <em>slow</em>, and <em>fragile</em> - and no amount of tinkering with clever setups and special approaches got past the fact that the tests took far far too long to run, and were fragile, and hampered rapid development.</p>

<p>Besides, they encouraged people to only think of “acceptance” in terms of “clicks and buttons” - where were the tests for “it should handle network failures gracefully” or “it should send exception logs to the auditing system”?</p>

<p>(I do think there’s value in <em>some</em> browser-driven end-to-end tests - there are bugs you can only catch that way.  But call them “smoke tests” or “end-to-end tests” or something, not “acceptance tests”.  If you want lots of tests of UI features, consider tools that test within the framework you use, like <a href="https://airbnb.io/enzyme/">enzyme</a> for react - you might want several “cheese slices” of UI tests - see <a href="https://github.com/mui-org/material-ui/blob/master/test/README.md">how the material UI tests are layered</a> for an example.)</p>

<p>In my opinion, acceptance criteria should be tested at <em>whatever level is most effective</em> for testing that requirement:</p>

<p><img src="/assets/images/2019-07-22-the-swiss-cheese-model-and-acceptance-tests/acceptance_tests.svg" alt="acceptance tests" /></p>

<p>If the acceptance criterion is “it should have the title <code class="language-plaintext highlighter-rouge">Fnord Motor Company</code>” then that can be a simple browser-based test against a stubbed back end.  If the acceptance criterion is “it should not accept a password shorter than 10 characters” then that might be a pair of unit tests, one to check that the UI validation is good, and one to check that the server-side validation is good. If the acceptance criterion is  “it should respond within 30ms under peak load” then that might be part of a performance test suite.</p>

<p>If you really feel the need to trace acceptance tests back to stories, you can probably work out a way to tag the tests and report on them somehow - but I’d ask, why bother?  Are you ever going to use that information?  Maybe it’s sufficient to just list the tests in the stories, and check they are there as part of signing the story as done, and not try to track the relationship beyond that.</p>

<h2 id="user-stories-are-a-point-in-time-tests-are-forever">User stories are a point in time, tests are forever</h2>

<p>Another problem with an Acceptance Test layer, is that user stories express the requirements at the time the story was written - usually with changes right up to the point of delivery.  At the time of finishing a story, acceptance tests will demonstrate it works - but the next story, or the one after that, will change the behaviour.  If your tests are too strongly structured around the stories, they can hamper change - you need to think “OK, it’s now the <code class="language-plaintext highlighter-rouge">Flawed Motor Company</code>, I need to find all the old acceptance criteria that might have tested for this”.</p>

<p>And again, this is especially time consuming if that test is in a UI-based test suite which takes a long time to run and is hard to debug.</p>

<p>If your acceptance test was just a note in the original story “this is demonstrated by the view-layer test XYZ” then you wouldn’t need to change anything except the single failing test - if there was ever a pressing need to prove the original acceptance criteria were checked,  a code archeologist could dig through version control history to find the test XYZ at the time the story was completed.</p>

<h2 id="the-qa-role-and-test-slices">The QA role and test slices</h2>

<p>This also highlights a common organisational problem - when there is an individual or team whose role is entirely Quality Assurance, and who tests independently to the rest of the team.</p>

<p>It is only natural that this kind of QA will tend to work at a single layer - without close collaboration with developers, it is next to impossible to work with unit test or other fast feedback layers.</p>

<p>And this is a reason why you shouldn’t test this way!  Don’t get me wrong, I’m not advocating getting rid of QA folks - a good QA specialist can be an invaluable team member.  But they should be working <em>with</em> the developers, to encourage them to write good tests, to spot the gaps in their test automation, and to provide a vital whole-project perspective on quality.</p>

<p>The “Accelerate” <sup id="fnref:accelerate:1" role="doc-noteref"><a href="#fn:accelerate" class="footnote" rel="footnote">1</a></sup> folks put it better than I can:</p>

<blockquote>
  <p>[context: talking about qualities of high performing teams]</p>

  <p>Developers primarily create and maintain acceptance tests, and they can easily
reproduce and fix them on their development workstations.</p>

  <p>It is interesting to note
that having automated tests primarily created and maintained either by QA or an
outsourced party is not correlated with IT performance.</p>

  <p>The theory behind this is
that when developers are involved in creating and maintaining acceptance tests,
there are two important effects. First, the code becomes more testable when
developers write tests. This is one of the main reasons why test-driven development
(TDD) is an important practice‚ as it forces developers to create more testable designs.
Second, when developers are responsible for the automated tests, they care more
about them and will invest more effort into maintaining and fixing them.</p>

</blockquote>

<blockquote>
  <p>None of this means that we should be getting rid of testers.  Testers serve an essential role in the software delivery lifecycle, performing manual testing such as exploratory, usability, and acceptance testing, and <em>helping to create and evolve suites of automated tests by working alongside developers</em>.</p>
</blockquote>

<p>The best QAs I’ve worked with were awesome assets to the team.  They didn’t write that many tests on their own though - instead they worked closely with developers to ensure that they understood their tests, that the tests covered as many cheese holes
as possible, and that the test fitted in with their big-picture vision of the overall testing suite - Were tests at the right level? Was enough being tested?  Were acceptance criteria being met? And what areas would still need exploratory testing?</p>

<h1 id="in-summary">In summary</h1>

<ul>
  <li>Write tests at as low a level as is sensible.</li>
  <li>Write tests that cover all the things that could go wrong.  Where you can.</li>
  <li>Don’t repeat yourself!</li>
  <li>Integrate continuously - and run all the tests on each commit.  Or as many as you can.</li>
  <li>Define acceptance criteria, and write acceptance tests, at the lowest level that makes sense.</li>
  <li>Let the QAs overview the tests, let the developers write them, work as a team.</li>
</ul>

<p>Comments? Questions? See the comments section at the bottom of this page.</p>

<hr />

<h2 id="various-references">Various references:</h2>

<p><a href="https://en.wikipedia.org/wiki/Swiss_cheese_model">Wikipedia</a> has an article about this metaphor in accident causation - this seems to date the idea back to 1990, when looking at layered threat mitigations</p>

<p><a href="https://blog.feabhas.com/2011/12/effective-testing-the-swiss-cheese-model/">Glennan Carnie - the Swiss Cheese Model</a> is a good introduction to the metaphor, but seems to talk more about static analysis than kinds of tests</p>

<p><a href="https://semblanceoffunctionality.com/swiss-cheese-model/">My colleague Sarah Hutchins wrote this article recently about the Swiss Cheese Model</a> - with a bit of a different focus, looking at strategic decisions about what to test where. <a href="https://vinodkumaar.wordpress.com/2018/03/05/swiss-cheese-model-to-understand-test-coverage/">Vinod Kumaar R’s article</a> has more useful perspectives.</p>

<p><a href="https://martinfowler.com/bliki/TestPyramid.html">Martin Fowler</a> has a good basic article about the Test Pyramid in his bliki, and the same site also has <a href="https://martinfowler.com/articles/practical-test-pyramid.html">Ham Vocke’s article with more practical examples</a></p>

<p>I also quite like <a href="https://gerg.dev/2018/05/testing-is-like-a-box-of-rocks/">Gregory Paciga’s “testing is like a box of rocks” metaphor</a></p>

<h2 id="image-sources">Image sources:</h2>

<p><a href="https://commons.wikimedia.org/wiki/File:Swiss_cheese.jpg">Swiss cheese</a> image by <a href="https://commons.wikimedia.org/wiki/User:Ekg917">Ekg917</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode" rel="license">CC BY-SA 4.0</a></p>

<p>Test Pyramid image by Abbe98 [<a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>], <a href="https://commons.wikimedia.org/wiki/File:Testing_Pyramid.svg">via Wikimedia Commons</a></p>

<h2 id="thanks">Thanks:</h2>

<ul>
  <li>to my colleagues <a href="https://twitter.com/googflow">Nishi Ningegowda</a>, <a href="https://twitter.com/jimbarritt">Jim Barritt</a>, <a href="https://twitter.com/staylward">Steve Aylward</a>, and of course <a href="https://twitter.com/becky_thorn">Becky Thorn Sietsma</a> for their useful feedback!</li>
</ul>

<h2 id="footnotes">Footnotes:</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:accelerate" role="doc-endnote">
      <p>Nicole Forsgren, Jez Humble &amp; Gene Kim - Accelerate <a href="https://itrevolution.com/book/accelerate/">https://itrevolution.com/book/accelerate/</a> <a href="#fnref:accelerate" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:accelerate:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#agile" class="page__taxonomy-item p-category" rel="tag">agile</a><span class="sep">, </span>
    
      <a href="/tags/#testing" class="page__taxonomy-item p-category" rel="tag">testing</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#software-development" class="page__taxonomy-item p-category" rel="tag">software development</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2020-01-20T08:13:00+00:00">January 20, 2020</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=The+swiss+cheese+model+and+acceptance+tests%20http%3A%2F%2Flocalhost%3A4000%2F2020%2F01%2F20%2Fthe-swiss-cheese-model-and-acceptance-tests.html" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2020%2F01%2F20%2Fthe-swiss-cheese-model-and-acceptance-tests.html" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2020%2F01%2F20%2Fthe-swiss-cheese-model-and-acceptance-tests.html" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/2019/04/08/ocr-hack-grabbing-text-from-the-screen-on-a-macbook.html" class="pagination--pager" title="OCR Hack - grabbing text from the screen on a Macbook
">Previous</a>
    
    
      <a href="/2020/02/10/multiple-git-identities.html" class="pagination--pager" title="multiple git identities
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You May Also Enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2023/10/14/blog-refresh" rel="permalink">New job, new blog!
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-10-14T00:00:00+01:00">October 14, 2023</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">I quit my job! Yes, after 12 amazing years at Thoughtworks, I decided it was time to move on.

I’m starting a new job in November - more on that later - but ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2022/12/01/folks-to-follow-on-mastodon.html" rel="permalink">Interesting folks to follow on Mastodon
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-12-01T09:27:00+00:00">December 1, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">My highly-idiosyncratic list of interesting people on Mastodon

I’ve been really enjoying moving from the crumbling mess of Twitter, to the chaotic federated...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2022/11/10/buying-minecoins-on-a-child-s-android-minecraft-account.html" rel="permalink">Buying minecoins on a child’s Android Minecraft account
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-11-10T11:01:00+00:00">November 10, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Sharing here as this was a world of pain, and maybe I can save someone else this pain.

Information is as of November 2022 - earlier advice on the web is wro...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2022/11/05/is-mastodon-a-twitter-replacement.html" rel="permalink">Is Mastodon a Twitter replacement?
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-11-05T21:10:00+00:00">November 5, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">It depends which bits of Twitter you want…
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://hachyderm.io/@korny" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-mastodon" aria-hidden="true"></i> Mastodon</a></li>
        
      
        
          <li><a href="https://github.com/kornysietsma" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Korny's Blog. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/2020/01/20/the-swiss-cheese-model-and-acceptance-tests.html";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/2020/01/20/the-swiss-cheese-model-and-acceptance-tests.html"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://kornyblog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
